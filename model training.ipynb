{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e134ab-9c83-409e-a4aa-c31b08e5a6be",
   "metadata": {},
   "source": [
    "DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5401d8b-7a75-410f-8db6-ce61cb57b1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path recognized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SIZE = (128, 128)  \n",
    "BATCH_SIZE = 32\n",
    "train_dir = 'D://new dataset//train'\n",
    "test_dir = 'D://new dataset//test'\n",
    "print(\"Path recognized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4cd3d-df23-4d61-a49f-7830577ad879",
   "metadata": {},
   "source": [
    "DATA AUGUMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbcafeef-45dc-4278-8cdc-4636acd69276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2351 images belonging to 2 classes.\n",
      "Found 810 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=30, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2, \n",
    "                                   shear_range=0.2, \n",
    "                                   zoom_range=0.2, \n",
    "                                   horizontal_flip=True, \n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                    target_size=IMAGE_SIZE, \n",
    "                                                    batch_size=BATCH_SIZE, \n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, \n",
    "                                                  target_size=IMAGE_SIZE, \n",
    "                                                  batch_size=BATCH_SIZE, \n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd324c77-66cc-4525-9900-863352deab69",
   "metadata": {},
   "source": [
    "DEFINING THE LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5488b6b8-600f-440e-934f-7c44b2cef867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    \n",
    "    layers.Conv2D(32, (3, 3), activation='relu'), layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'), layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3,3), activation='relu'), layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),  \n",
    "\n",
    "    layers.Dense(2, activation='softmax')  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae614c23-6fbd-4164-8bd3-9a69a5974494",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "447e0ff3-1ef0-4035-bcee-f8f30154fc3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 581ms/step - accuracy: 0.9527 - loss: 0.2336 - val_accuracy: 0.9725 - val_loss: 0.0912\n",
      "Epoch 2/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.9688 - loss: 0.1026 - val_accuracy: 0.9712 - val_loss: 0.0962\n",
      "Epoch 3/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 555ms/step - accuracy: 0.9628 - loss: 0.1058 - val_accuracy: 0.9712 - val_loss: 0.0637\n",
      "Epoch 4/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 0.9688 - loss: 0.1688 - val_accuracy: 0.9725 - val_loss: 0.0703\n",
      "Epoch 5/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 560ms/step - accuracy: 0.9606 - loss: 0.1216 - val_accuracy: 0.9712 - val_loss: 0.0684\n",
      "Epoch 6/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0338 - val_accuracy: 0.9712 - val_loss: 0.0685\n",
      "Epoch 7/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 537ms/step - accuracy: 0.9683 - loss: 0.0748 - val_accuracy: 0.9775 - val_loss: 0.0572\n",
      "Epoch 8/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0508 - val_accuracy: 0.9737 - val_loss: 0.0590\n",
      "Epoch 9/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 546ms/step - accuracy: 0.9669 - loss: 0.0767 - val_accuracy: 0.9737 - val_loss: 0.0642\n",
      "Epoch 10/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 0.9688 - loss: 0.0788 - val_accuracy: 0.9737 - val_loss: 0.0647\n",
      "Epoch 11/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 546ms/step - accuracy: 0.9700 - loss: 0.0723 - val_accuracy: 0.9775 - val_loss: 0.0610\n",
      "Epoch 12/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.9688 - loss: 0.4445 - val_accuracy: 0.9787 - val_loss: 0.0548\n",
      "Epoch 13/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 534ms/step - accuracy: 0.9687 - loss: 0.0950 - val_accuracy: 0.9750 - val_loss: 0.0666\n",
      "Epoch 14/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.0152 - val_accuracy: 0.9737 - val_loss: 0.0735\n",
      "Epoch 15/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 549ms/step - accuracy: 0.9675 - loss: 0.0705 - val_accuracy: 0.9750 - val_loss: 0.0648\n",
      "Epoch 16/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 0.9750 - val_loss: 0.0649\n",
      "Epoch 17/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 572ms/step - accuracy: 0.9772 - loss: 0.0607 - val_accuracy: 0.9812 - val_loss: 0.0584\n",
      "Epoch 18/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 79ms/step - accuracy: 0.9688 - loss: 0.0724 - val_accuracy: 0.9812 - val_loss: 0.0507\n",
      "Epoch 19/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 541ms/step - accuracy: 0.9790 - loss: 0.0658 - val_accuracy: 0.9837 - val_loss: 0.0480\n",
      "Epoch 20/20\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0354 - val_accuracy: 0.9825 - val_loss: 0.0473\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 232ms/step - accuracy: 0.9807 - loss: 0.0547\n",
      "Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_generator, \n",
    "                    steps_per_epoch=train_generator.samples // BATCH_SIZE, \n",
    "                    epochs=20, \n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=test_generator.samples // BATCH_SIZE)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5cb905d9-62f1-4deb-8d68-d7fb0087a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to model.keras\n"
     ]
    }
   ],
   "source": [
    "save_path = 'model.keras'\n",
    "model.save(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac335ea9-3114-41c6-aa87-fa0b7f023ee0",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20a2bf47-b796-434c-addf-76cd2a2bafd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\n",
      "Predicted Class: car\n",
      "Confidence: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "model = load_model('model.keras')\n",
    "IMAGE_SIZE = (128, 128)\n",
    "image_path = 'C://Users//rajes//OneDrive//Desktop//swift.jpg'\n",
    "image = load_img(image_path, target_size=IMAGE_SIZE)\n",
    "image_array = img_to_array(image)\n",
    "image_array = image_array / 255.0  \n",
    "image_array = np.expand_dims(image_array, axis=0) \n",
    "predictions = model.predict(image_array)\n",
    "class_labels = ['car', 'cat']  \n",
    "predicted_class = class_labels[np.argmax(predictions)]\n",
    "confidence = np.max(predictions)\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
